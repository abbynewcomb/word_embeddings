{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c32932-e1ea-44dd-b034-10d336b52928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import requests as rq\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim import models\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e09066-9d89-4463-9b43-842936f19c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fox = Word2Vec.load(\"./models/fox.model\")\n",
    "model_reuters = Word2Vec.load(\"./models/reuters.model\")\n",
    "model_the_hill = Word2Vec.load(\"./models/the_hill.model\")\n",
    "model_cnn = Word2Vec.load(\"./models/cnn.model\")\n",
    "model_nyt = Word2Vec.load(\"./models/nyt.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b3efb0a-72a0-4701-8008-78b54b3a9f48",
   "metadata": {
    "id": "aVtxa-YW9tg6"
   },
   "outputs": [],
   "source": [
    "adjectives = ['disorganized', 'devious', 'impressionable', 'circumspect', 'impassive', \n",
    "             'aimless', 'effeminate', 'unfathomable', 'fickle', 'unprincipled', 'inoffensive', \n",
    "             'reactive', 'providential', 'resentful', 'bizarre', 'impractical',\n",
    "             'sarcastic', 'misguided', 'imitative', 'pedantic', 'venomous', 'erratic', 'insecure', \n",
    "             'resourceful', 'neurotic', 'forgiving', 'profligate', 'whimsical', 'assertive', \n",
    "             'incorruptible', 'individualistic', 'faithless', 'disconcerting', 'barbaric', \n",
    "             'hypnotic', 'vindictive', 'observant', 'dissolute', 'frightening', 'complacent', \n",
    "             'boisterous', 'pretentious', 'disobedient', 'tasteless', 'sedentary', \n",
    "             'sophisticated', 'regimental', 'mellow', 'deceitful', 'impulsive', 'playful', \n",
    "             'sociable', 'methodical', 'willful', 'idealistic', 'boyish', 'callous', 'pompous', \n",
    "             'unchanging', 'crafty', 'punctual', 'compassionate', 'intolerant', 'challenging', \n",
    "             'scornful', 'possessive', 'conceited', 'imprudent', 'dutiful', 'lovable', \n",
    "             'disloyal', 'dreamy', 'appreciative', 'forgetful', 'unrestrained', 'forceful', \n",
    "             'submissive', 'predatory', 'fanatical', 'illogical', 'tidy', 'aspiring', 'studious', \n",
    "             'adaptable', 'conciliatory', 'artful', 'thoughtless', 'deceptive', 'frugal', \n",
    "             'reflective', 'insulting', 'unreliable', 'stoic', 'hysterical', 'rustic', \n",
    "             'inhibited', 'outspoken', 'unhealthy', 'ascetic', 'skeptical', 'painstaking', \n",
    "             'contemplative', 'leisurely', 'sly', 'mannered', 'outrageous', 'lyrical', \n",
    "             'placid', 'cynical', 'irresponsible', 'vulnerable', 'arrogant', 'persuasive', \n",
    "             'perverse', 'steadfast', 'crisp', 'envious', 'naive', 'greedy', 'presumptuous', \n",
    "             'obnoxious', 'irritable', 'dishonest', 'discreet', 'sporting', 'hateful', \n",
    "             'ungrateful', 'frivolous', 'reactionary', 'skillful', 'cowardly', 'sordid', \n",
    "             'adventurous', 'dogmatic', 'intuitive', 'bland', 'indulgent', 'discontented', \n",
    "             'dominating', 'articulate', 'fanciful', 'discouraging', 'treacherous', \n",
    "             'repressed', 'moody', 'sensual', 'unfriendly', 'optimistic', 'clumsy', \n",
    "             'contemptible', 'focused', 'haughty', 'morbid', 'disorderly', 'considerate', \n",
    "             'humorous', 'preoccupied', 'airy', 'impersonal', 'cultured', 'trusting', \n",
    "             'respectful', 'scrupulous', 'scholarly', 'superstitious', 'tolerant', \n",
    "             'realistic', 'malicious', 'irrational', 'sane', 'colorless', 'masculine', \n",
    "             'witty', 'inert', 'prejudiced', 'fraudulent', 'blunt', 'childish', 'brittle', \n",
    "             'disciplined', 'responsive', 'courageous', 'bewildered', 'courteous', \n",
    "             'stubborn', 'aloof', 'sentimental', 'athletic', 'extravagant', 'brutal', \n",
    "             'manly', 'cooperative', 'unstable', 'youthful', 'timid', 'amiable', 'retiring', \n",
    "             'fiery', 'confidential', 'relaxed', 'imaginative', 'mystical', 'shrewd', \n",
    "             'conscientious', 'monstrous', 'grim', 'questioning', 'lazy', 'dynamic', \n",
    "             'gloomy', 'troublesome', 'abrupt', 'eloquent', 'dignified', 'hearty', 'gallant', \n",
    "             'benevolent', 'maternal', 'paternal', 'patriotic', 'aggressive', 'competitive', \n",
    "             'elegant', 'flexible', 'gracious', 'energetic', 'tough', 'contradictory', \n",
    "             'shy', 'careless', 'cautious', 'polished', 'sage', 'tense', 'caring', \n",
    "             'suspicious', 'sober', 'neat', 'transparent', 'disturbing', 'passionate', \n",
    "             'obedient', 'crazy', 'restrained', 'fearful', 'daring', 'prudent', 'demanding', \n",
    "             'impatient', 'cerebral', 'calculating', 'amusing', 'honorable', 'casual',\n",
    "             'sharing', 'selfish', 'ruined', 'spontaneous', 'admirable', 'conventional', \n",
    "             'cheerful', 'solitary', 'upright', 'stiff', 'enthusiastic', 'petty', 'dirty', \n",
    "             'subjective', 'heroic', 'stupid', 'modest', 'impressive', 'orderly', 'ambitious', \n",
    "             'protective', 'silly', 'alert', 'destructive', 'exciting', 'crude', 'ridiculous', \n",
    "             'subtle', 'mature', 'creative', 'coarse', 'passive', 'oppressed', 'accessible', \n",
    "             'charming', 'clever', 'decent', 'miserable', 'superficial', 'shallow', 'stern', \n",
    "             'winning', 'balanced', 'emotional', 'rigid', 'invisible', 'desperate', 'cruel', \n",
    "             'romantic', 'agreeable', 'hurried', 'sympathetic', 'solemn', 'systematic', \n",
    "             'vague', 'peaceful', 'humble', 'dull', 'expedient', 'loyal', 'decisive', \n",
    "             'arbitrary', 'earnest', 'confident', 'conservative', 'foolish', 'moderate', \n",
    "             'helpful', 'delicate', 'gentle', 'dedicated', 'hostile', 'generous', 'reliable', \n",
    "             'dramatic', 'precise', 'calm', 'healthy', 'attractive', 'artificial', \n",
    "             'progressive', 'odd', 'confused', 'rational', 'brilliant', 'intense', \n",
    "             'genuine', 'mistaken', 'driving', 'stable', 'objective', 'sensitive', \n",
    "             'neutral', 'strict', 'angry', 'profound', 'smooth', 'ignorant', 'thorough', \n",
    "             'logical', 'intelligent', 'extraordinary', 'experimental', 'steady', \n",
    "             'formal', 'faithful', 'curious', 'reserved', 'honest', 'busy', 'educated', \n",
    "             'liberal', 'friendly', 'efficient', 'sweet', 'surprising', 'mechanical', \n",
    "             'clean', 'critical', 'criminal', 'soft', 'proud', 'quiet', 'weak', 'anxious', \n",
    "             'solid', 'complex', 'grand', 'warm', 'slow', 'false', 'extreme', 'narrow', \n",
    "             'dependent', 'wise', 'organized', 'pure', 'directed', 'dry', 'obvious', 'popular', \n",
    "             'capable', 'secure', 'active', 'independent', 'ordinary', 'fixed', 'practical', \n",
    "             'serious', 'fair', 'understanding', 'constant', 'cold', 'responsible', 'deep', \n",
    "             'religious', 'private', 'simple', 'physical', 'original', 'working', 'strong', \n",
    "             'modern', 'determined', 'open', 'political', 'difficult', 'knowledge', 'kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98a03429-2b5d-4f25-b62e-de97350a90aa",
   "metadata": {
    "id": "R0zwzzLJE15h"
   },
   "outputs": [],
   "source": [
    "testing_words = [\"gun\", \"metoo\", \"america\", \"tax\", \"abortion\", \"democrat\", \"republican\", \"nra\", \n",
    "                 \"trump\", \"biden\", \"china\", \"gay\", \"lesbian\"]\n",
    "\n",
    "women_list = ['daughter', 'mother', 'woman', 'girl', 'female', \n",
    "              'sister', 'aunt', 'niece']\n",
    "men_list = ['son', 'father', 'man', 'boy', 'male', 'brother',\n",
    "            'uncle', 'nephew']\n",
    "\n",
    "straight_list = ['heterosexual', 'straight']\n",
    "lgbt_list = ['gay', 'trans', 'bisexual', 'lesbian', 'homosexual', 'transgender']\n",
    "\n",
    "religion_list = ['god', 'church', 'temple', 'holy', 'synagogue', 'religion', 'sacred', 'jesus', 'worship']\n",
    "secular_list = ['secular', 'atheist', 'agnostic']\n",
    "\n",
    "d_list = ['democrat', 'liberal', 'right', 'pelosi', 'obama']\n",
    "r_list = ['republican', 'conservative', 'left', 'trump', 'mcconnell']\n",
    "\n",
    "school_subjects = ['english', 'math', 'art', 'science', 'history', 'music', 'geography', 'gym', 'drama', 'biology',\n",
    "                   'chemistry', 'physics', 'technology', 'foreign languages', 'social studies', 'philosophy', 'design',\n",
    "                   'literature', 'algebra', 'geometry']\n",
    "\n",
    "#careers = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1181cf8d-9f88-496a-81a0-67f1d79ca852",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqDduKH06o4J",
    "outputId": "cfb00c98-aee8-40c7-94bd-ef07e8a1a8ae"
   },
   "outputs": [],
   "source": [
    "def print_adj_dists(news_source, model, word_list, compare_wordlist):    \n",
    "    for word in word_list:\n",
    "      word_dists = []\n",
    "      for ind in range(len(compare_wordlist)):\n",
    "        adj = compare_wordlist[ind]\n",
    "        try:\n",
    "          dist = model.wv.distance(word, adj)\n",
    "          word_dists.append([news_source, word, adj, dist])\n",
    "        except:\n",
    "          word_dists.append([news_source, word, adj, None])\n",
    "\n",
    "      df = pd.DataFrame(word_dists, columns=['source','word', 'adj', 'dist'])\n",
    "\n",
    "      print(df.shape)\n",
    "\n",
    "      df_sorted = df.sort_values(by='dist', ascending=True)\n",
    "      print(df_sorted.head(5))\n",
    "        \n",
    "def print_word_dists_each_model(wordlist, compare_wordlist):\n",
    "    print_adj_dists(\"Fox News\", model_fox, wordlist, compare_wordlist)\n",
    "    print_adj_dists(\"Reuters\", model_reuters, wordlist, compare_wordlist)\n",
    "    print_adj_dists(\"The Hill\", model_the_hill, wordlist, compare_wordlist)\n",
    "    print_adj_dists(\"NYT\", model_nyt, wordlist, compare_wordlist)\n",
    "    print_adj_dists(\"CNN\", model_cnn, wordlist, compare_wordlist)\n",
    "    \n",
    "def find_avg_loc(model, wordlist):\n",
    "    avg = [0]*100\n",
    "    for i in range(0, len(wordlist)):\n",
    "        avg += model.wv.get_vector(wordlist[i])\n",
    "    avg /= len(wordlist)\n",
    "    return avg\n",
    "\n",
    "def find_avg_dists(news_source, model, wordlist_name, wordlist, compare_wordlist, print_output=True):\n",
    "    word_dists = []\n",
    "    avg_loc = find_avg_loc(model, wordlist)\n",
    "    for ind in range(len(compare_wordlist)):\n",
    "      compare_word = compare_wordlist[ind]\n",
    "      try: \n",
    "        dist = abs(np.linalg.norm(avg_loc-model.wv.get_vector(compare_word)))\n",
    "        word_dists.append([news_source, wordlist_name, compare_word, dist])\n",
    "      except:\n",
    "        word_dists.append([news_source, wordlist_name, compare_word, None])\n",
    "\n",
    "    df = pd.DataFrame(word_dists, columns=['news_source', 'wordlist_name', 'compare_word', 'dist'])\n",
    "    \n",
    "    if print_output:\n",
    "        df_sorted = df.sort_values(by='dist', ascending=True)\n",
    "        print(df.shape)\n",
    "        print(df_sorted.head())\n",
    "    return df\n",
    "\n",
    "def find_avg_dists_each_model(wordlist_name, wordlist, compare_wordlist):\n",
    "    find_avg_dists(\"Fox\", model_fox, wordlist_name, wordlist, compare_wordlist)\n",
    "    #find_avg_dists(\"Reuters\", model_reuters, wordlist_name, wordlist, compare_wordlist)\n",
    "    #find_avg_dists(\"The Hill\", model_the_hill, wordlist_name, wordlist, compare_wordlist)\n",
    "    #find_avg_dists(\"NYT\", model_nyt, wordlist_name, wordlist, compare_wordlist)\n",
    "    #find_avg_dists(\"CNN\", model_cnn, wordlist_name, wordlist, compare_wordlist)\n",
    "    \n",
    "def find_avg_dists_compare(news_source, model, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist):\n",
    "    dists1 = find_avg_dists(news_source, model, wordlist_name1, wordlist1, compare_wordlist, print_output=False)\n",
    "    dists2 = find_avg_dists(news_source, model, wordlist_name2, wordlist2, compare_wordlist, print_output=False)\n",
    "\n",
    "    print(\"\"\"\\nif difference between dists is positive, association is stronger \n",
    "with wordlist1. if negative, association is stronger with wordlist2\\n\"\"\")\n",
    "    \n",
    "    diffs = pd.DataFrame()\n",
    "    diffs['news_source'] = dists1['news_source']\n",
    "    diffs['wordlist1'] = wordlist_name1\n",
    "    diffs['wordlist2'] = wordlist_name2\n",
    "    diffs['compare_word'] = dists1['compare_word']\n",
    "    diffs['diff'] = dists1['dist']-dists2['dist']\n",
    "    \n",
    "    df_sorted = diffs.sort_values(by='diff', ascending=False)\n",
    "    print(df_sorted.head())\n",
    "    if len(df_sorted) > 5:\n",
    "        df_sorted = diffs.sort_values(by='diff', ascending=True)\n",
    "        print(df_sorted.head())\n",
    "        \n",
    "def find_avg_dists_compare_each_model(wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist):\n",
    "    find_avg_dists_compare(\"Fox News\", model_fox, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_dists_compare(\"Reuters\", model_reuters, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_dists_compare(\"The Hill\", model_the_hill, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_dists_compare(\"NYT\", model_nyt, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_dists_compare(\"CNN\", model_cnn, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)  \n",
    "        \n",
    "def find_avg_relative_dists(news_source, model, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist):\n",
    "    dists1 = find_avg_dists(news_source, model, wordlist_name1, wordlist1, compare_wordlist, print_output=False)\n",
    "    dists2 = find_avg_dists(news_source, model, wordlist_name2, wordlist2, compare_wordlist, print_output=False)\n",
    "    \n",
    "    rel1 = dists1['dist']/(dists1['dist']+dists2['dist'])\n",
    "    rel2 = dists2['dist']/(dists1['dist']+dists2['dist'])\n",
    "    \n",
    "    rels = pd.DataFrame()\n",
    "    rels['news_source'] = dists1['news_source']\n",
    "    rels['compare_word'] = dists1['compare_word']\n",
    "    rels['wordlist1'] = wordlist_name1\n",
    "    rels['d1/d1+d2'] = rel1\n",
    "    rels['wordlist2'] = wordlist_name2\n",
    "    rels['d2/d1+d2'] = rel2\n",
    "    rels['compare_word'] = dists1['compare_word']\n",
    "    \n",
    "    print(rels)\n",
    "    \n",
    "def find_rel_dists_compare_each_model(wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist):\n",
    "    find_avg_relative_dists(\"Fox News\", model_fox, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_relative_dists(\"Reuters\", model_reuters, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_relative_dists(\"The Hill\", model_the_hill, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_relative_dists(\"NYT\", model_nyt, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)\n",
    "    find_avg_relative_dists(\"CNN\", model_cnn, wordlist_name1, wordlist1, wordlist_name2, wordlist2, compare_wordlist)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb98701e-67b2-47ca-8934-40394613ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "if difference between dists is positive, association is stronger \n",
      "with wordlist1. if negative, association is stronger with wordlist2\n",
      "\n",
      "    news_source wordlist1 wordlist2  compare_word      diff\n",
      "365    Fox News         r         s       liberal  0.940206\n",
      "318    Fox News         r         s  conservative  0.825804\n",
      "334    Fox News         r         s   progressive  0.812022\n",
      "320    Fox News         r         s      moderate  0.766365\n",
      "87     Fox News         r         s     deceptive  0.742832\n",
      "    news_source wordlist1 wordlist2 compare_word      diff\n",
      "359    Fox News         r         s     faithful -0.265766\n",
      "20     Fox News         r         s     venomous -0.225071\n",
      "380    Fox News         r         s      complex -0.201097\n",
      "268    Fox News         r         s       heroic -0.167153\n",
      "236    Fox News         r         s        sober -0.142112\n"
     ]
    }
   ],
   "source": [
    "find_avg_dists_compare_each_model(\"r\", religion_list, \"s\", secular_list, adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "homeless-success",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0    Fox News   submissive     woman  0.521833       man  0.478167\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0     Reuters   submissive     woman  0.516779       man  0.483221\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0    The Hill   submissive     woman  0.526956       man  0.473044\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0         NYT   submissive     woman  0.505395       man  0.494605\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0         CNN   submissive     woman   0.51115       man   0.48885\n"
     ]
    }
   ],
   "source": [
    "find_rel_dists_compare_each_model(\"woman\", women_list, \"man\", men_list, [\"submissive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "organizational-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0    Fox News         love     lgbtq  0.510260  straight  0.489740\n",
      "1    Fox News       family     lgbtq  0.492736  straight  0.507264\n",
      "2    Fox News      america     lgbtq  0.510355  straight  0.489645\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0     Reuters         love     lgbtq  0.516145  straight  0.483855\n",
      "1     Reuters       family     lgbtq  0.515259  straight  0.484741\n",
      "2     Reuters      america     lgbtq  0.509871  straight  0.490129\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0    The Hill         love     lgbtq  0.546670  straight  0.453330\n",
      "1    The Hill       family     lgbtq  0.517181  straight  0.482819\n",
      "2    The Hill      america     lgbtq  0.537367  straight  0.462633\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0         NYT         love     lgbtq  0.531965  straight  0.468035\n",
      "1         NYT       family     lgbtq  0.521382  straight  0.478618\n",
      "2         NYT      america     lgbtq  0.510069  straight  0.489931\n",
      "  news_source compare_word wordlist1  d1/d1+d2 wordlist2  d2/d1+d2\n",
      "0         CNN         love     lgbtq  0.536061  straight  0.463939\n",
      "1         CNN       family     lgbtq  0.514964  straight  0.485036\n",
      "2         CNN      america     lgbtq  0.517712  straight  0.482288\n"
     ]
    }
   ],
   "source": [
    "find_rel_dists_compare_each_model(\"lgbtq\", lgbt_list, \"straight\", straight_list, [\"love\", \"family\", \"america\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f995d0-5eeb-4b06-8eab-59bf47d36072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 4)\n",
      "       source    word           adj      dist\n",
      "334  Fox News  ocasio   progressive  0.365418\n",
      "16   Fox News  ocasio     sarcastic  0.465474\n",
      "264  Fox News  ocasio  enthusiastic  0.471049\n",
      "62   Fox News  ocasio    intolerant  0.496467\n",
      "365  Fox News  ocasio       liberal  0.497042\n",
      "(423, 4)\n",
      "      source    word           adj      dist\n",
      "137  Reuters  ocasio  discontented  0.388344\n",
      "64   Reuters  ocasio      scornful  0.414708\n",
      "365  Reuters  ocasio       liberal  0.426324\n",
      "54   Reuters  ocasio    idealistic  0.427847\n",
      "212  Reuters  ocasio      eloquent  0.430797\n",
      "(423, 4)\n",
      "       source    word          adj      dist\n",
      "334  The Hill  ocasio  progressive  0.525036\n",
      "365  The Hill  ocasio      liberal  0.542625\n",
      "182  The Hill  ocasio   bewildered  0.556756\n",
      "86   The Hill  ocasio  thoughtless  0.586852\n",
      "133  The Hill  ocasio     dogmatic  0.591986\n",
      "(423, 4)\n",
      "    source    word           adj      dist\n",
      "334    NYT  ocasio   progressive  0.304151\n",
      "365    NYT  ocasio       liberal  0.445957\n",
      "133    NYT  ocasio      dogmatic  0.526874\n",
      "137    NYT  ocasio  discontented  0.529837\n",
      "419    NYT  ocasio     political  0.558443\n",
      "(423, 4)\n",
      "    source    word          adj      dist\n",
      "334    CNN  ocasio  progressive  0.342480\n",
      "3      CNN  ocasio  circumspect  0.495550\n",
      "67     CNN  ocasio    imprudent  0.517822\n",
      "365    CNN  ocasio      liberal  0.519986\n",
      "40     CNN  ocasio   boisterous  0.524526\n"
     ]
    }
   ],
   "source": [
    "print_word_dists_each_model(['ocasio'], adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8f381197-2c4b-43d4-9324-d44b48ee80e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 4)\n",
      "       source      word          adj      dist\n",
      "179  Fox News  shooting  disciplined  0.486670\n",
      "38   Fox News  shooting  frightening  0.510087\n",
      "92   Fox News  shooting        stoic  0.525629\n",
      "304  Fox News  shooting      hurried  0.540596\n",
      "43   Fox News  shooting    tasteless  0.542874\n",
      "(423, 4)\n",
      "      source      word       adj      dist\n",
      "92   Reuters  shooting     stoic  0.496251\n",
      "187  Reuters  shooting  athletic  0.525407\n",
      "64   Reuters  shooting  scornful  0.532825\n",
      "268  Reuters  shooting    heroic  0.535264\n",
      "59   Reuters  shooting    crafty  0.552180\n",
      "(423, 4)\n",
      "       source      word           adj      dist\n",
      "115  The Hill  shooting         crisp  0.521371\n",
      "149  The Hill  shooting  contemptible  0.606837\n",
      "86   The Hill  shooting   thoughtless  0.609420\n",
      "182  The Hill  shooting    bewildered  0.609676\n",
      "304  The Hill  shooting       hurried  0.615771\n",
      "(423, 4)\n",
      "    source      word           adj      dist\n",
      "239    NYT  shooting    disturbing  0.521737\n",
      "117    NYT  shooting         naive  0.551564\n",
      "397    NYT  shooting        active  0.581808\n",
      "137    NYT  shooting  discontented  0.590521\n",
      "33     NYT  shooting      barbaric  0.595252\n",
      "(423, 4)\n",
      "    source      word            adj      dist\n",
      "7      CNN  shooting   unfathomable  0.441806\n",
      "130    CNN  shooting       cowardly  0.460592\n",
      "86     CNN  shooting    thoughtless  0.523777\n",
      "32     CNN  shooting  disconcerting  0.533247\n",
      "239    CNN  shooting     disturbing  0.539093\n"
     ]
    }
   ],
   "source": [
    "print_word_dists_each_model(['shooting'], adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc158751-5cf9-43dd-9a97-2a746cd8670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 4)\n",
      "       source   word          adj      dist\n",
      "131  Fox News  metoo       sordid  0.397127\n",
      "81   Fox News  metoo     aspiring  0.448535\n",
      "96   Fox News  metoo    outspoken  0.494659\n",
      "257  Fox News  metoo  spontaneous  0.505243\n",
      "75   Fox News  metoo     forceful  0.507450\n",
      "(423, 4)\n",
      "      source   word         adj      dist\n",
      "93   Reuters  metoo  hysterical  0.410461\n",
      "6    Reuters  metoo  effeminate  0.440745\n",
      "62   Reuters  metoo  intolerant  0.446195\n",
      "297  Reuters  metoo   emotional  0.467436\n",
      "239  Reuters  metoo  disturbing  0.469760\n",
      "(423, 4)\n",
      "       source   word          adj      dist\n",
      "257  The Hill  metoo  spontaneous  0.451610\n",
      "200  The Hill  metoo  imaginative  0.477864\n",
      "212  The Hill  metoo     eloquent  0.485439\n",
      "133  The Hill  metoo     dogmatic  0.486279\n",
      "93   The Hill  metoo   hysterical  0.489868\n",
      "(423, 4)\n",
      "    source   word           adj      dist\n",
      "93     NYT  metoo    hysterical  0.494415\n",
      "128    NYT  metoo   reactionary  0.513331\n",
      "133    NYT  metoo      dogmatic  0.522175\n",
      "64     NYT  metoo      scornful  0.548170\n",
      "119    NYT  metoo  presumptuous  0.549713\n",
      "(423, 4)\n",
      "    source   word          adj      dist\n",
      "212    CNN  metoo     eloquent  0.445763\n",
      "10     CNN  metoo  inoffensive  0.466384\n",
      "40     CNN  metoo   boisterous  0.489204\n",
      "54     CNN  metoo   idealistic  0.505109\n",
      "184    CNN  metoo     stubborn  0.506617\n"
     ]
    }
   ],
   "source": [
    "print_word_dists_each_model(['metoo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74896ee-711e-4247-a9d5-7f16efc369c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(423, 4)\n",
      "       source      word            adj      dist\n",
      "286  Fox News  american      oppressed  0.430025\n",
      "143  Fox News  american      repressed  0.470680\n",
      "203  Fox News  american  conscientious  0.476445\n",
      "183  Fox News  american      courteous  0.496688\n",
      "303  Fox News  american      agreeable  0.500179\n",
      "(423, 4)\n",
      "      source      word           adj      dist\n",
      "182  Reuters  american    bewildered  0.555218\n",
      "64   Reuters  american      scornful  0.565878\n",
      "0    Reuters  american  disorganized  0.575307\n",
      "20   Reuters  american      venomous  0.578273\n",
      "187  Reuters  american      athletic  0.583312\n",
      "(423, 4)\n",
      "       source      word             adj      dist\n",
      "44   The Hill  american       sedentary  0.452374\n",
      "13   The Hill  american       resentful  0.462171\n",
      "216  The Hill  american      benevolent  0.483714\n",
      "399  The Hill  american        ordinary  0.485799\n",
      "2    The Hill  american  impressionable  0.493461\n",
      "(423, 4)\n",
      "    source      word           adj      dist\n",
      "137    NYT  american  discontented  0.485100\n",
      "399    NYT  american      ordinary  0.500217\n",
      "46     NYT  american    regimental  0.513777\n",
      "67     NYT  american     imprudent  0.528417\n",
      "219    NYT  american     patriotic  0.533163\n",
      "(423, 4)\n",
      "    source      word          adj      dist\n",
      "128    CNN  american  reactionary  0.470266\n",
      "216    CNN  american   benevolent  0.491696\n",
      "399    CNN  american     ordinary  0.494045\n",
      "36     CNN  american    observant  0.500838\n",
      "290    CNN  american       decent  0.526771\n"
     ]
    }
   ],
   "source": [
    "print_word_dists_each_model(['american'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d87ec-47b5-4569-bb1d-76d936a21cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
